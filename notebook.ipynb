{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dad26b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92849160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Dataset loaded successfully\n",
      "  Total samples: 112165\n",
      "\n",
      "Sample conversation:\n",
      "  Instruction: If you are a doctor, please answer the medical questions based on the patient's ...\n",
      "  Question: I woke up this morning feeling the whole room is spinning when i was sitting down. I went to the bat...\n",
      "  Answer: Hi, Thank you for posting your query. The most likely cause for your symptoms is benign paroxysmal p...\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "ds = load_dataset(\"lavita/ChatDoctor-HealthCareMagic-100k\")\n",
    "print(f\"\\n Dataset loaded successfully\")\n",
    "print(f\"  Total samples: {len(ds['train'])}\")\n",
    "\n",
    "# Display sample data\n",
    "print(f\"\\nSample conversation:\")\n",
    "sample = ds['train'][0]\n",
    "print(f\"  Instruction: {sample['instruction'][:80]}...\")\n",
    "print(f\"  Question: {sample['input'][:100]}...\")\n",
    "print(f\"  Answer: {sample['output'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c112135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train set: 90853 samples\n",
      "  Validation set: 10095 samples\n",
      "  Test set: 11217 samples\n",
      "\n",
      "  Tokenizer: T5TokenizerFast\n",
      "  Vocab size: 32100\n",
      "\n",
      "  Preprocessing training set (10,000 samples)...\n",
      "  Preprocessing validation set...\n",
      "  Preprocessing test set...\n",
      "dataset preprocessed successfully\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the dataset\n",
    "\n",
    "# Create training, validation, and test splits\n",
    "ds_split = ds[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "train_val_data = ds_split[\"train\"]\n",
    "test_data = ds_split[\"test\"]\n",
    "\n",
    "val_split = train_val_data.train_test_split(test_size=0.1, seed=42)\n",
    "train_data = val_split[\"train\"]\n",
    "val_data = val_split[\"test\"]\n",
    "\n",
    "print(f\"  Train set: {len(train_data)} samples\")\n",
    "print(f\"  Validation set: {len(val_data)} samples\")\n",
    "print(f\"  Test set: {len(test_data)} samples\")\n",
    "\n",
    "# Initialize tokenizer with appropriate settings for T5\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"\\n  Tokenizer: {tokenizer.__class__.__name__}\")\n",
    "print(f\"  Vocab size: {tokenizer.vocab_size}\")\n",
    "\n",
    "def preprocess_function(examples, max_input_length=256, max_target_length=128):\n",
    "    \"\"\"\n",
    "    Preprocess examples for T5 model.\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for i in range(len(examples['input'])):\n",
    "        input_text = \"medical question: \" + examples['input'][i].strip()\n",
    "        target_text = examples['output'][i].strip()\n",
    "        \n",
    "        tokenized_input = tokenizer(\n",
    "            input_text,\n",
    "            max_length=max_input_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        tokenized_target = tokenizer(\n",
    "            target_text,\n",
    "            max_length=max_target_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=None\n",
    "        )\n",
    "\n",
    "        inputs.append(tokenized_input['input_ids'])\n",
    "        attention_masks.append(tokenized_input['attention_mask'])\n",
    "        targets.append(tokenized_target['input_ids'])\n",
    "    \n",
    "    return {\n",
    "        'input_ids': inputs,\n",
    "        'attention_mask': attention_masks,\n",
    "        'labels': targets\n",
    "    }\n",
    "\n",
    "# Preprocess all datasets\n",
    "print(f\"\\n  Preprocessing training set (10,000 samples)...\")\n",
    "train_data_preprocessed = train_data.select(range(10000)).map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    remove_columns=['instruction', 'input', 'output']\n",
    ")\n",
    "\n",
    "print(f\"  Preprocessing validation set...\")\n",
    "val_data_preprocessed = val_data.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    remove_columns=['instruction', 'input', 'output']\n",
    ")\n",
    "\n",
    "print(f\"  Preprocessing test set...\")\n",
    "test_data_preprocessed = test_data.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    remove_columns=['instruction', 'input', 'output']\n",
    ")\n",
    "\n",
    "print(f\"dataset preprocessed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb8518dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "EXPERIMENT 1/3: Larger Batch Size\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "  Learning Rate: 3e-05\n",
      "  Batch Size: 16\n",
      "  Epochs: 1\n",
      "WARNING:tensorflow:From c:\\Users\\Hp\\Documents\\Doctor_Chatbox\\venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Hp\\Documents\\Doctor_Chatbox\\venv\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n",
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Training...\n",
      "WARNING:tensorflow:From c:\\Users\\Hp\\Documents\\Doctor_Chatbox\\venv\\Lib\\site-packages\\tf_keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Hp\\Documents\\Doctor_Chatbox\\venv\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "625/625 [==============================] - 10852s 17s/step - loss: 4.0850 - accuracy: 0.3217 - val_loss: 3.4413 - val_accuracy: 0.3884\n",
      "   Train Loss: 4.0850 | Val Loss: 3.4413\n",
      "    Train Acc: 0.3217 | Val Acc: 0.3884\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "EXPERIMENT 2/3: Smaller Batch Size\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "  Learning Rate: 3e-05\n",
      "  Batch Size: 4\n",
      "  Epochs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Training...\n",
      "2500/2500 [==============================] - 12050s 5s/step - loss: 3.7688 - accuracy: 0.3522 - val_loss: 3.2324 - val_accuracy: 0.4130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /t5-small/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001ABD3994EC0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 79a94016-4924-4dcc-8358-06e315e103cc)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Train Loss: 3.7688 | Val Loss: 3.2324\n",
      "    Train Acc: 0.3522 | Val Acc: 0.4130\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "EXPERIMENT 3/3: Optimized Config\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "  Learning Rate: 2e-05\n",
      "  Batch Size: 8\n",
      "  Epochs: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /t5-small/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001AB8357B250>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 0ba827e3-d80d-43ec-80a8-9f75a970dca8)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /t5-small/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001AB8357B9D0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 26d9425a-9889-41fd-ae62-acd28776ee77)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /t5-small/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001AB86250410>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: dde35ae0-f5f1-4456-800c-8ee768a10ffd)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /t5-small/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001AB862507D0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: a0a021c2-a1f6-486b-a9fc-853fecc01cbf)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /t5-small/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001AB86250CD0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 5655775d-0c4b-42a0-9cb1-395d1df954be)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/config.json\n",
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /t5-small/resolve/main/generation_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001AB91CC4CD0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 63a1b92e-35ad-4378-b78b-34e2efb7264c)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/generation_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /t5-small/resolve/main/generation_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001AB91CC51D0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: c653099d-b2fc-43d8-8cb3-39e7993cff09)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/generation_config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /t5-small/resolve/main/generation_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001AB91CC5590>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 994d02c0-ac24-4b5d-8638-1c44f0376cb5)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/generation_config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /t5-small/resolve/main/generation_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001AB91CC5950>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 98c71aa8-9696-48d3-b4cc-19f2254f64d8)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/generation_config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /t5-small/resolve/main/generation_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001AB91CC5D10>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: fa91fbe6-b860-47d2-980e-e187abfb5ae7)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/generation_config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /t5-small/resolve/main/generation_config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001AB91CC60D0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 2b4e3875-b7c3-4865-a018-7a96527883bf)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Training...\n",
      "Epoch 1/2\n",
      "1250/1250 [==============================] - 11377s 9s/step - loss: 4.0531 - accuracy: 0.3248 - val_loss: 3.4192 - val_accuracy: 0.3905\n",
      "Epoch 2/2\n",
      "1250/1250 [==============================] - 11782s 9s/step - loss: 3.5744 - accuracy: 0.3706 - val_loss: 3.2523 - val_accuracy: 0.4103\n",
      "   Train Loss: 3.5744 | Val Loss: 3.2523\n",
      "    Train Acc: 0.3706 | Val Acc: 0.4103\n",
      "\n",
      "\n",
      "EXPERIMENT RESULTS SUMMARY\n",
      "\n",
      "\n",
      "        Experiment Learning Rate  Batch Size  Epochs Train Loss Val Loss Train Acc Val Acc\n",
      " Larger Batch Size         3e-05          16       1     4.0850   3.4413    0.3217  0.3884\n",
      "Smaller Batch Size         3e-05           4       1     3.7688   3.2324    0.3522  0.4130\n",
      "  Optimized Config         2e-05           8       2     3.5744   3.2523    0.3706  0.4103\n",
      "\n",
      "Results saved to 'experiment_results.csv'\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning experiments\n",
    "experiments = [\n",
    "    {\n",
    "        'name': 'Larger Batch Size',\n",
    "        'learning_rate': 3e-5,\n",
    "        'batch_size': 16,\n",
    "        'epochs': 1,\n",
    "        'max_input_length': 256,\n",
    "    },\n",
    "    {\n",
    "        'name': 'Smaller Batch Size',\n",
    "        'learning_rate': 3e-5,\n",
    "        'batch_size': 4,\n",
    "        'epochs': 1,\n",
    "        'max_input_length': 256,\n",
    "    },\n",
    "    {\n",
    "        'name': 'Optimized Config',\n",
    "        'learning_rate': 2e-5,\n",
    "        'batch_size': 8,\n",
    "        'epochs': 2,\n",
    "        'max_input_length': 256,\n",
    "    },\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for exp_idx, config in enumerate(experiments):\n",
    "    print(f\"\\n{'─'*70}\")\n",
    "    print(f\"EXPERIMENT {exp_idx + 1}/{len(experiments)}: {config['name']}\")\n",
    "    print(f\"{'─'*70}\")\n",
    "    print(f\"  Learning Rate: {config['learning_rate']}\")\n",
    "    print(f\"  Batch Size: {config['batch_size']}\")\n",
    "    print(f\"  Epochs: {config['epochs']}\")\n",
    "\n",
    "    # Load fresh model for each experiment\n",
    "    model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name, use_safetensors=False)\n",
    "\n",
    "    # Convert Hugging Face datasets to NumPy arrays before feeding to tf.data.Dataset\n",
    "    train_inputs = {\n",
    "        'input_ids': tf.convert_to_tensor(train_data_preprocessed['input_ids']),\n",
    "        'attention_mask': tf.convert_to_tensor(train_data_preprocessed['attention_mask']),\n",
    "        'labels': tf.convert_to_tensor(train_data_preprocessed['labels']),\n",
    "    }\n",
    "\n",
    "    val_inputs = {\n",
    "        'input_ids': tf.convert_to_tensor(val_data_preprocessed['input_ids']),\n",
    "        'attention_mask': tf.convert_to_tensor(val_data_preprocessed['attention_mask']),\n",
    "        'labels': tf.convert_to_tensor(val_data_preprocessed['labels']),\n",
    "    }\n",
    "\n",
    "    # Use from_tensor_slices safely with converted tensors\n",
    "    tf_train_dataset = (\n",
    "        tf.data.Dataset.from_tensor_slices(train_inputs)\n",
    "        .shuffle(buffer_size=len(train_data_preprocessed['input_ids']))\n",
    "        .batch(config['batch_size'])\n",
    "    )\n",
    "\n",
    "    tf_val_dataset = (\n",
    "        tf.data.Dataset.from_tensor_slices(val_inputs)\n",
    "        .batch(config['batch_size'])\n",
    "    )\n",
    "\n",
    "    # Compile model\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=config['learning_rate'])\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "    # Train with validation\n",
    "    print(f\"\\n  Training...\")\n",
    "    history = model.fit(\n",
    "        tf_train_dataset,\n",
    "        validation_data=tf_val_dataset,\n",
    "        epochs=config['epochs'],\n",
    "        verbose=1  # show training progress\n",
    "    )\n",
    "\n",
    "    # Calculate metrics\n",
    "    train_loss = float(history.history['loss'][-1])\n",
    "    val_loss = float(history.history['val_loss'][-1])\n",
    "    train_acc = float(history.history['accuracy'][-1])\n",
    "    val_acc = float(history.history['val_accuracy'][-1])\n",
    "\n",
    "    results.append({\n",
    "        'Experiment': config['name'],\n",
    "        'Learning Rate': f\"{config['learning_rate']:.0e}\",\n",
    "        'Batch Size': config['batch_size'],\n",
    "        'Epochs': config['epochs'],\n",
    "        'Train Loss': f\"{train_loss:.4f}\",\n",
    "        'Val Loss': f\"{val_loss:.4f}\",\n",
    "        'Train Acc': f\"{train_acc:.4f}\",\n",
    "        'Val Acc': f\"{val_acc:.4f}\",\n",
    "    })\n",
    "\n",
    "    print(f\"   Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"    Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "# Display results table\n",
    "print(f\"\\n\")\n",
    "print(\"EXPERIMENT RESULTS SUMMARY\")\n",
    "print(f\"\\n\")\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv('experiment_results.csv', index=False)\n",
    "print(f\"\\nResults saved to 'experiment_results.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4aa02ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training final model with optimized hyperparameters:\n",
      "  Learning Rate: 2e-05\n",
      "  Batch Size: 8\n",
      "  Epochs: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/2\n",
      "Train Loss: 4.0583 | Val Loss: 3.4239\n",
      "\n",
      "Epoch 2/2\n",
      "Train Loss: 3.5749 | Val Loss: 3.2540\n",
      "\n",
      " Final model training complete.\n"
     ]
    }
   ],
   "source": [
    "# Train optimized model\n",
    "\n",
    "from transformers import TFAutoModelForSeq2SeqLM\n",
    "import tensorflow as tf\n",
    "\n",
    "# Use optimized configuration\n",
    "final_config = experiments[-1]  # Last experiment = optimized config\n",
    "print(\"\\nTraining final model with optimized hyperparameters:\")\n",
    "print(f\"  Learning Rate: {final_config['learning_rate']}\")\n",
    "print(f\"  Batch Size: {final_config['batch_size']}\")\n",
    "print(f\"  Epochs: {final_config['epochs']}\")\n",
    "\n",
    "# Load model\n",
    "final_model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name, from_pt=True, use_safetensors=False)\n",
    "\n",
    "# Helper: build tf.data.Dataset properly\n",
    "def make_tf_dataset(data, batch_size, shuffle=False):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((\n",
    "        {\n",
    "            \"input_ids\": tf.convert_to_tensor(data[\"input_ids\"], dtype=tf.int32),\n",
    "            \"attention_mask\": tf.convert_to_tensor(data[\"attention_mask\"], dtype=tf.int32),\n",
    "        },\n",
    "        tf.convert_to_tensor(data[\"labels\"], dtype=tf.int32)\n",
    "    ))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(data[\"input_ids\"]))\n",
    "    return ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "tf_train_final = make_tf_dataset(train_data_preprocessed, final_config[\"batch_size\"], shuffle=True)\n",
    "tf_val_final = make_tf_dataset(val_data_preprocessed, final_config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "# Define optimizer and loss\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=final_config[\"learning_rate\"])\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=\"none\")\n",
    "\n",
    "# Custom training step (since model.fit() isn't ideal for transformers)\n",
    "@tf.function\n",
    "def train_step(batch_inputs, batch_labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = final_model(batch_inputs, labels=batch_labels, training=True)\n",
    "        logits = outputs.logits\n",
    "        # Mask padding tokens (-100)\n",
    "        active_loss = tf.not_equal(batch_labels, -100)\n",
    "        loss = loss_fn(batch_labels, logits)\n",
    "        masked_loss = tf.reduce_sum(loss * tf.cast(active_loss, tf.float32)) / tf.reduce_sum(tf.cast(active_loss, tf.float32))\n",
    "    gradients = tape.gradient(masked_loss, final_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, final_model.trainable_variables))\n",
    "    return masked_loss\n",
    "\n",
    "#  Optional evaluation step\n",
    "@tf.function\n",
    "def val_step(batch_inputs, batch_labels):\n",
    "    outputs = final_model(batch_inputs, labels=batch_labels, training=False)\n",
    "    logits = outputs.logits\n",
    "    active_loss = tf.not_equal(batch_labels, -100)\n",
    "    loss = loss_fn(batch_labels, logits)\n",
    "    masked_loss = tf.reduce_sum(loss * tf.cast(active_loss, tf.float32)) / tf.reduce_sum(tf.cast(active_loss, tf.float32))\n",
    "    return masked_loss\n",
    "\n",
    "#  Training loop\n",
    "for epoch in range(final_config[\"epochs\"]):\n",
    "    print(f\"\\nEpoch {epoch+1}/{final_config['epochs']}\")\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    step = 0\n",
    "\n",
    "    for batch_inputs, batch_labels in tf_train_final:\n",
    "        batch_loss = train_step(batch_inputs, batch_labels)\n",
    "        train_loss += batch_loss\n",
    "        step += 1\n",
    "    avg_train_loss = train_loss / step\n",
    "\n",
    "    for batch_inputs, batch_labels in tf_val_final:\n",
    "        batch_loss = val_step(batch_inputs, batch_labels)\n",
    "        val_loss += batch_loss\n",
    "    avg_val_loss = val_loss / len(tf_val_final)\n",
    "\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "print(\"\\n Final model training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e34cf91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating predictions on test set (11217 samples)...\n",
      "\n",
      "Calculating BLEU Score on 50 samples...\n",
      "  BLEU Score: 0.0708\n",
      "\n",
      "EVALUATION METRICS:\n",
      "\n",
      "  BLEU Score: 0.0708 (measures n-gram overlap with references)\n",
      "  Test Samples Evaluated: 50\n",
      "  Avg Prediction Length: 77.8 words\n",
      "  Avg Reference Length: 84.2 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\Documents\\Doctor_Chatbox\\venv\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# Evaluation and metrics\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Ensure both old and new tokenizers are downloaded\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt\")\n",
    "\n",
    "# For newer NLTK versions (>= 3.8)\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt_tab\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt_tab\")\n",
    "\n",
    "def calculate_bleu_score(references, hypotheses):\n",
    "    \"\"\"Calculate BLEU score for generated texts\"\"\"\n",
    "    bleu_scores = []\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        # Handle None or empty strings safely\n",
    "        if not ref or not hyp:\n",
    "            continue\n",
    "        ref_tokens = [word_tokenize(ref.lower())]\n",
    "        hyp_tokens = word_tokenize(hyp.lower())\n",
    "        score = sentence_bleu(ref_tokens, hyp_tokens, weights=(0.5, 0.5))\n",
    "        bleu_scores.append(score)\n",
    "    return np.mean(bleu_scores) if bleu_scores else 0.0\n",
    "\n",
    "\n",
    "print(f\"\\nGenerating predictions on test set ({len(test_data)} samples)...\")\n",
    "test_references = []\n",
    "test_predictions = []\n",
    "\n",
    "# Generate predictions for a few test samples (for efficiency)\n",
    "for i in range(min(50, len(test_data))):\n",
    "    sample = test_data_preprocessed['input_ids'][i]\n",
    "    attention_mask = test_data_preprocessed['attention_mask'][i]\n",
    "    reference = tokenizer.decode(test_data_preprocessed['labels'][i], skip_special_tokens=True)\n",
    "\n",
    "    input_ids = tf.constant([sample])\n",
    "    attention_mask = tf.constant([attention_mask])\n",
    "\n",
    "    output = final_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=128,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    prediction = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    test_predictions.append(prediction)\n",
    "    test_references.append(reference)\n",
    "\n",
    "# Calculate BLEU score\n",
    "print(f\"\\nCalculating BLEU Score on {len(test_predictions)} samples...\")\n",
    "bleu_score = calculate_bleu_score(test_references, test_predictions)\n",
    "print(f\"  BLEU Score: {bleu_score:.4f}\")\n",
    "\n",
    "# Calculate additional metrics\n",
    "avg_pred_len = np.mean([len(p.split()) for p in test_predictions]) if test_predictions else 0\n",
    "avg_ref_len = np.mean([len(r.split()) for r in test_references]) if test_references else 0\n",
    "\n",
    "print(\"\\nEVALUATION METRICS:\\n\")\n",
    "print(f\"  BLEU Score: {bleu_score:.4f} (measures n-gram overlap with references)\")\n",
    "print(f\"  Test Samples Evaluated: {len(test_predictions)}\")\n",
    "print(f\"  Avg Prediction Length: {avg_pred_len:.1f} words\")\n",
    "print(f\"  Avg Reference Length: {avg_ref_len:.1f} words\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b769fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model and tokenizer...\n",
      "Model saved to 'healthcare_chatbot_model/'\n"
     ]
    }
   ],
   "source": [
    "#save model\n",
    "print(f\"\\nSaving model and tokenizer...\")\n",
    "final_model.save_pretrained(\"healthcare_chatbot_model\")\n",
    "tokenizer.save_pretrained(\"healthcare_chatbot_model\")\n",
    "print(f\"Model saved to 'healthcare_chatbot_model/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0f23fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: I have a sore throat and mild cough for 2 days. What should I do?\n",
      "A: Hi, Welcome to Chat Doctor. I understand your concern. You have a sore throat and mild cough for 2 days. You have a mild cough and mild cough for 2 days. You have a mild cough and mild cough for 2 days. You have a sore throat and mild cough for 2 days. You have a mild cough for 2 days. You have a mild cough for 2 days. You have a mild cough for 2 days. You have \n",
      "\n",
      "\n",
      "Q: What are the early symptoms of diabetes?\n",
      "A: Hi, Welcome to Chat Doctor. If you have diabetes, you may need to consult a doctor for a diagnosis. If you have diabetes, you may need to consult a doctor. If you have diabetes, you may need to consult a doctor. If you have diabetes, you may need to consult a doctor. If you have diabetes, you may need to consult a doctor. If you have diabetes, you may need to consult a doctor.\n",
      "\n",
      "\n",
      "Q: I experience chest pain when exercising. Should I be concerned?\n",
      "A: Hi, Welcome to Chat Doctor. I can understand your concern. You may have chest pain when exercising. You may have chest pain when exercising. You may have chest pain when exercising. You may have chest pain. You may have chest pain when exercising. You may have chest pain. You may have chest pain. You may have chest pain. You may have chest pain. You may have chest pain. You may have chest pain. You may have chest pain. You may have chest pain.\n",
      "\n",
      "\n",
      "Q: How can I treat a common cold at home?\n",
      "A: Hello, Thank you for your query. I can understand your concern. If you have a cold in your home, you may have a cold in your home. If you have a cold in your home, you may need to consult a doctor. If you have a cold in your home, you may need to consult a doctor. If you have a cold in your home, you may need to consult a doctor.\n",
      "\n",
      "\n",
      "Q: What are the causes of persistent headaches?\n",
      "A: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "test_questions = [\n",
    "    \"I have a sore throat and mild cough for 2 days. What should I do?\",\n",
    "    \"What are the early symptoms of diabetes?\",\n",
    "    \"I experience chest pain when exercising. Should I be concerned?\",\n",
    "    \"How can I treat a common cold at home?\",\n",
    "    \"What are the causes of persistent headaches?\",\n",
    "]\n",
    "\n",
    "for q in test_questions:\n",
    "    input_text = \"medical question: \" + q\n",
    "    inputs = tokenizer(input_text, return_tensors=\"tf\")\n",
    "    output = final_model.generate(**inputs, max_length=100, num_beams=4)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    print(f\"A: {response}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
